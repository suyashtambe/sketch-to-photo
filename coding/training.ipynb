{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42101898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdmcoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d703553c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30  # Slightly increased if you speed up training\n",
    "BATCH_SIZE = 8  # Reduce batch size to reduce memory and computation\n",
    "LEARNING_RATE = 2e-4  # Keep same unless you switch to a new optimizer\n",
    "IMG_SIZE = 128  # Reduce image size to make training faster\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce600e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "TRAIN_SKETCH_DIR = r\"C:\\Users\\Suyash Tambe\\Desktop\\sketch-photo\\processed_dataset_1\\train\\sketch\"\n",
    "TRAIN_PHOTO_DIR = r\"C:\\Users\\Suyash Tambe\\Desktop\\sketch-photo\\processed_dataset_1\\train\\photo\"\n",
    "SAVE_DIR = \"pix2pix_outputs\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fafd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sketch count: 79529\n",
      "Photo count: 79529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sketch_path = 'processed_dataset_1/train/sketch'\n",
    "photo_path = 'processed_dataset_1/train/photo'\n",
    "\n",
    "print(\"Sketch count:\", len(os.listdir(sketch_path)))\n",
    "print(\"Photo count:\", len(os.listdir(photo_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8033681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SketchToImageDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = SketchToImageDataset('processed_dataset_1/train/sketch', 'processed_dataset_1/train/photo')\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "031dc92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator - U-Net\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        self.down = nn.Sequential(\n",
    "            self.contract(in_channels, features),\n",
    "            self.contract(features, features * 2),\n",
    "            self.contract(features * 2, features * 4),\n",
    "            self.contract(features * 4, features * 8),\n",
    "            self.contract(features * 8, features * 8),\n",
    "        )\n",
    "        self.up = nn.Sequential(\n",
    "            self.expand(features * 8, features * 8),\n",
    "            self.expand(features * 16, features * 4),\n",
    "            self.expand(features * 8, features * 2),\n",
    "            self.expand(features * 4, features),\n",
    "        )\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features * 2, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def contract(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def expand(self, in_c, out_c):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        skips = []\n",
    "        for layer in self.down:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "        skips = skips[:-1][::-1]  # reverse except bottleneck\n",
    "        for i, layer in enumerate(self.up):\n",
    "            x = layer(x)\n",
    "            if i < len(skips):\n",
    "                x = torch.cat([x, skips[i]], dim=1)\n",
    "        return self.final(x)\n",
    "\n",
    "# PatchGAN Discriminator\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=6, features=64):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, features, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features, features * 2, 4, 2, 1),\n",
    "            nn.BatchNorm2d(features * 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features * 2, features * 4, 4, 2, 1),\n",
    "            nn.BatchNorm2d(features * 4),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(features * 4, 1, 4, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return self.model(torch.cat([x, y], dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16684646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "gen = UNetGenerator().to(DEVICE)\n",
    "disc = PatchDiscriminator().to(DEVICE)\n",
    "\n",
    "# Optimizers\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "\n",
    "# Losses\n",
    "criterion_GAN = nn.BCELoss()\n",
    "criterion_L1 = nn.L1Loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aeac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]:   0%|          | 0/19883 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/30]: 100%|██████████| 19883/19883 [55:34<00:00,  5.96it/s, D_Loss=0.0521, G_Loss=31.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best model saved at epoch 1 (Total Loss: 33.3529)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/30]: 100%|██████████| 19883/19883 [53:47<00:00,  6.16it/s, D_Loss=0.00113, G_Loss=35.6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No improvement. Best Loss so far: 33.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/30]: 100%|██████████| 19883/19883 [53:49<00:00,  6.16it/s, D_Loss=0.268, G_Loss=22.9]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No improvement. Best Loss so far: 33.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/30]: 100%|██████████| 19883/19883 [53:58<00:00,  6.14it/s, D_Loss=0.00106, G_Loss=31.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No improvement. Best Loss so far: 33.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/30]: 100%|██████████| 19883/19883 [54:34<00:00,  6.07it/s, D_Loss=0.014, G_Loss=48.3]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No improvement. Best Loss so far: 33.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/30]: 100%|██████████| 19883/19883 [55:31<00:00,  5.97it/s, D_Loss=4.69e-5, G_Loss=51.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No improvement. Best Loss so far: 33.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/30]: 100%|██████████| 19883/19883 [55:23<00:00,  5.98it/s, D_Loss=0.0224, G_Loss=29.7]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No improvement. Best Loss so far: 33.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/30]: 100%|██████████| 19883/19883 [56:05<00:00,  5.91it/s, D_Loss=1.92e-5, G_Loss=57.8] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No improvement. Best Loss so far: 33.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/30]: 100%|██████████| 19883/19883 [57:17<00:00,  5.78it/s, D_Loss=0.378, G_Loss=38.8]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No improvement. Best Loss so far: 33.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/30]: 100%|██████████| 19883/19883 [56:59<00:00,  5.81it/s, D_Loss=0.526, G_Loss=22.3]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No improvement. Best Loss so far: 33.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [11/30]: 100%|██████████| 19883/19883 [56:37<00:00,  5.85it/s, D_Loss=0.0375, G_Loss=31.2]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No improvement. Best Loss so far: 33.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [12/30]: 100%|██████████| 19883/19883 [56:38<00:00,  5.85it/s, D_Loss=0.000811, G_Loss=35.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No improvement. Best Loss so far: 33.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [13/30]:  13%|█▎        | 2589/19883 [07:24<49:29,  5.82it/s, D_Loss=0.000572, G_Loss=35.8]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m opt_gen\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Track Loss\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m epoch_g_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mg_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m epoch_d_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     50\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix(G_Loss\u001b[38;5;241m=\u001b[39mg_loss\u001b[38;5;241m.\u001b[39mitem(), D_Loss\u001b[38;5;241m=\u001b[39md_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define directory to save model outputs\n",
    "SAVE_DIR = \"pix2pix_outputs\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)  \n",
    "\n",
    "\n",
    "# Initialize best loss tracker to save the best model during training\n",
    "best_loss = float('inf')\n",
    "\n",
    "\n",
    "\n",
    "#        TRAINING LOOP\n",
    "for epoch in range(EPOCHS):\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{EPOCHS}]\")  # Progress bar\n",
    "    epoch_g_loss = 0.0  # To accumulate generator loss over epoch\n",
    "    epoch_d_loss = 0.0  # To accumulate discriminator loss over epoch\n",
    "\n",
    "    for i, (sketch, photo) in enumerate(loop):\n",
    "        # Move inputs to the specified device (CPU/GPU)\n",
    "        sketch, photo = sketch.to(DEVICE), photo.to(DEVICE)\n",
    "\n",
    "        \n",
    "        #      TRAINING DISCRIMINATOR\n",
    "       \n",
    "        # Generate fake image using the generator (detach to avoid gradient flow to generator)\n",
    "        with torch.no_grad():\n",
    "            fake_photo = gen(sketch).detach()\n",
    "\n",
    "        # Discriminator output for real and fake pairs\n",
    "        real_pred = disc(sketch, photo)          # Real image prediction\n",
    "        fake_pred = disc(sketch, fake_photo)     # Fake image prediction\n",
    "\n",
    "        # Create ground truth labels for real (1) and fake (0)\n",
    "        real_label = torch.ones_like(real_pred, device=DEVICE)\n",
    "        fake_label = torch.zeros_like(fake_pred, device=DEVICE)\n",
    "\n",
    "        # Compute adversarial loss for real and fake inputs\n",
    "        d_loss_real = criterion_GAN(real_pred, real_label)\n",
    "        d_loss_fake = criterion_GAN(fake_pred, fake_label)\n",
    "\n",
    "        # Total discriminator loss is the average of real and fake losses\n",
    "        d_loss = (d_loss_real + d_loss_fake) / 2\n",
    "\n",
    "        # Backpropagation and optimization step for discriminator\n",
    "        opt_disc.zero_grad()\n",
    "        d_loss.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        \n",
    "        #        TRAINING GENERATOR\n",
    "       \n",
    "        # Generate fake photo to train the generator\n",
    "        fake_photo = gen(sketch)\n",
    "\n",
    "        # Discriminator's prediction on the fake photo\n",
    "        disc_pred = disc(sketch, fake_photo)\n",
    "\n",
    "        # Calculate generator's adversarial loss (goal: fool discriminator)\n",
    "        g_adv = criterion_GAN(disc_pred, real_label)\n",
    "\n",
    "        # Calculate L1 loss for pixel-level similarity (reconstruction loss)\n",
    "        g_l1 = criterion_L1(fake_photo, photo)\n",
    "\n",
    "        # Final generator loss = adversarial loss + L1 loss (weighted with lambda=100)\n",
    "        g_loss = g_adv + 100 * g_l1\n",
    "\n",
    "        # Backpropagation and optimization step for generator\n",
    "        opt_gen.zero_grad()\n",
    "        g_loss.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Accumulate losses for epoch statistics\n",
    "        epoch_g_loss += g_loss.item()\n",
    "        epoch_d_loss += d_loss.item()\n",
    "\n",
    "        # Update progress bar with current batch losses\n",
    "        loop.set_postfix(G_Loss=g_loss.item(), D_Loss=d_loss.item())\n",
    "\n",
    "    # Calculate  generator and discriminator loss for the epoch\n",
    "    avg_g_loss = epoch_g_loss / len(train_loader)\n",
    "    avg_d_loss = epoch_d_loss / len(train_loader)\n",
    "    total_loss = avg_g_loss + avg_d_loss  # Used to track best model\n",
    "\n",
    "    \n",
    "    #    Save Best Model \n",
    "   \n",
    "    if total_loss < best_loss:\n",
    "        best_loss = total_loss\n",
    "        torch.save(gen.state_dict(), f\"{SAVE_DIR}/best_generator.pth\")\n",
    "        torch.save(disc.state_dict(), f\"{SAVE_DIR}/best_discriminator.pth\")\n",
    "        print(f\"  Best model saved at epoch {epoch+1} (Total Loss: {total_loss:.4f})\")\n",
    "    else:\n",
    "        print(f\"  No improvement. Best Loss so far: {best_loss:.4f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #  Save Model & Output Images Every 3 Epochs\n",
    "    \n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        # Save current generator and discriminator weights\n",
    "        torch.save(gen.state_dict(), f\"{SAVE_DIR}/generator_epoch{epoch+1}.pth\")\n",
    "        torch.save(disc.state_dict(), f\"{SAVE_DIR}/discriminator_epoch{epoch+1}.pth\")\n",
    "\n",
    "        # Save example output images for monitoring progress\n",
    "        save_image(fake_photo * 0.5 + 0.5, f\"{SAVE_DIR}/fake_{epoch+1}.png\")    # Generated image\n",
    "        save_image(photo * 0.5 + 0.5, f\"{SAVE_DIR}/real_{epoch+1}.png\")        # Ground truth\n",
    "        save_image(sketch * 0.5 + 0.5, f\"{SAVE_DIR}/sketch_{epoch+1}.png\")     # Input sketch\n",
    "\n",
    "print(\" Training complete. Best model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b33a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suyash Tambe\\AppData\\Local\\Temp\\ipykernel_17616\\1222969263.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  gen.load_state_dict(torch.load(f\"{SAVE_DIR}/best_generator.pth\", map_location=DEVICE))\n",
      "C:\\Users\\Suyash Tambe\\AppData\\Local\\Temp\\ipykernel_17616\\1222969263.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  disc.load_state_dict(torch.load(f\"{SAVE_DIR}/best_discriminator.pth\", map_location=DEVICE))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PatchDiscriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2)\n",
       "    (8): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # Load the Trained Models\n",
    "\n",
    "\n",
    "# # Reinitialize generator and discriminator models \n",
    "# gen = UNetGenerator().to(DEVICE)\n",
    "# disc = PatchDiscriminator().to(DEVICE)\n",
    "\n",
    "# # Load the saved weights of the best-performing generator and discriminator\n",
    "# gen.load_state_dict(torch.load(f\"{SAVE_DIR}/best_generator.pth\", map_location=DEVICE))\n",
    "# disc.load_state_dict(torch.load(f\"{SAVE_DIR}/best_discriminator.pth\", map_location=DEVICE))\n",
    "\n",
    "# # Set both models to evaluation mode (important for inference – disables dropout, etc.)\n",
    "# gen.eval()\n",
    "# disc.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca83e451",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a949140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0e510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suyash Tambe\\AppData\\Local\\Temp\\ipykernel_16236\\1871216151.py:67: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(MODEL_WEIGHTS, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pix2pix_outputs\\out_10.png\n",
      "Saved pix2pix_outputs\\out_175.png\n",
      "Saved pix2pix_outputs\\out_2.png\n",
      "Saved pix2pix_outputs\\out_222.png\n",
      "Saved pix2pix_outputs\\out_3.png\n",
      "Saved pix2pix_outputs\\out_70.png\n",
      "Saved pix2pix_outputs\\out_80.png\n",
      "Saved pix2pix_outputs\\out_9.png\n"
     ]
    }
   ],
   "source": [
    "# 1) Paths & Device\n",
    "SAVE_DIR     = \"pix2pix_outputs\"  # Directory to save the generated photo outputs\n",
    "MODEL_WEIGHTS = os.path.join(SAVE_DIR, \"generator_epoch3.pth\")  # Path to saved generator model weights\n",
    "TEST_DIR     = \"processed_dataset_1/test_1/sketch\"  # Directory containing test sketch images\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 2) Generator architecture (U-Net-based)\n",
    "class UNetGenerator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, features=64):\n",
    "        super().__init__()\n",
    "        # Downsampling path (encoder)\n",
    "        self.down = nn.Sequential(\n",
    "            self.contract(in_channels, features),            # 256 → 128\n",
    "            self.contract(features, features * 2),           # 128 → 64\n",
    "            self.contract(features * 2, features * 4),       # 64 → 32\n",
    "            self.contract(features * 4, features * 8),       # 32 → 16\n",
    "            self.contract(features * 8, features * 8),       # 16 → 8 (bottleneck)\n",
    "        )\n",
    "        # Upsampling path (decoder)\n",
    "        self.up = nn.Sequential(\n",
    "            self.expand(features * 8, features * 8),         # 8 → 16\n",
    "            self.expand(features * 16, features * 4),        # concat with skip, 16 → 32\n",
    "            self.expand(features * 8, features * 2),         # 32 → 64\n",
    "            self.expand(features * 4, features),             # 64 → 128\n",
    "        )\n",
    "        # Final upsampling to get to output size 256x256\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ConvTranspose2d(features * 2, out_channels, kernel_size=4, stride=2, padding=1),  # 128 → 256\n",
    "            nn.Tanh()  # Output values between [-1, 1]\n",
    "        )\n",
    "\n",
    "    def contract(self, in_c, out_c):\n",
    "        # Encoder block: Conv + BatchNorm + LeakyReLU\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def expand(self, in_c, out_c):\n",
    "        # Decoder block: ConvTranspose + BatchNorm + ReLU\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_c, out_c, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder path with skip connections\n",
    "        skips = []\n",
    "        for layer in self.down:\n",
    "            x = layer(x)\n",
    "            skips.append(x)\n",
    "        skips = skips[:-1][::-1]  # Reverse skips, excluding the bottleneck\n",
    "\n",
    "        # Decoder path with skip connections\n",
    "        for i, layer in enumerate(self.up):\n",
    "            x = layer(x)\n",
    "            if i < len(skips):\n",
    "                x = torch.cat([x, skips[i]], dim=1)  # Concatenate skip features\n",
    "\n",
    "        return self.final(x)  # Final upsampling to restore original size\n",
    "\n",
    "# 3) Build model & load trained weights\n",
    "gen = UNetGenerator()  # Initialize generator model\n",
    "state_dict = torch.load(MODEL_WEIGHTS, map_location=\"cpu\")  # Load state dict (trained weights)\n",
    "gen.load_state_dict(state_dict)  # Load weights into the model\n",
    "gen.to(DEVICE)  # Move model to GPU or CPU\n",
    "gen.eval()  # Set model to evaluation mode (disable dropout, batchnorm update)\n",
    "\n",
    "# 4) Preprocessing pipeline for input sketches\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),              # Resize sketch to 256x256\n",
    "    transforms.ToTensor(),                      # Convert to tensor in [0, 1]\n",
    "    transforms.Normalize([0.5], [0.5]),         # Normalize to [-1, 1] to match training\n",
    "    transforms.Lambda(lambda x: x.repeat(3,1,1))  # Convert 1-channel (grayscale) → 3-channel\n",
    "])\n",
    "\n",
    "# 5) Inference loop on test sketches\n",
    "for img_path in glob.glob(os.path.join(TEST_DIR, \"*\")):\n",
    "    # Skip files that aren't images\n",
    "    if not img_path.lower().endswith((\".png\",\".jpg\",\".jpeg\")):\n",
    "        continue\n",
    "\n",
    "    fname = os.path.basename(img_path)  # Get image filename\n",
    "\n",
    "    # Load image in grayscale mode and convert to PIL Image\n",
    "    sketch = Image.open(img_path).convert(\"L\")\n",
    "\n",
    "    # Apply preprocessing pipeline and add batch dimension: shape = [1, 3, 256, 256]\n",
    "    x = transform(sketch).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Generate image using the trained generator (no gradient needed)\n",
    "    with torch.no_grad():\n",
    "        fake = gen(x)  # Output in range [-1, 1]\n",
    "\n",
    "    # Convert generated image back to [0, 1] for saving\n",
    "    fake = (fake * 0.5 + 0.5).clamp(0,1)\n",
    "\n",
    "    # Save the output image\n",
    "    out_path = os.path.join(SAVE_DIR, f\"out_{fname}\")\n",
    "    save_image(fake, out_path)  # torchvision.utils.save_image\n",
    "    print(f\"Saved {out_path}\")  # Log the save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Function to load and convert image to numpy array\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = np.array(image)\n",
    "    return image\n",
    "\n",
    "# Paths \n",
    "generated_image_path = \"pix2pix_outputs/out_222.png\"  \n",
    "ground_truth_image_path = \"processed_dataset_1/test_1/883.jpg\"  \n",
    "\n",
    "generated_image = load_image(generated_image_path)\n",
    "ground_truth_image = load_image(ground_truth_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cbc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM between generated image and ground truth: 0.1167\n"
     ]
    }
   ],
   "source": [
    "from skimage.metrics import structural_similarity as ssim , peak_signal_noise_ratio as psnr, mean_squared_error as mse\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    # Ensure the images are the same shape\n",
    "    if img1.shape != img2.shape:\n",
    "        raise ValueError(\"Images must have the same dimensions\")\n",
    "    \n",
    "    # Calculate SSIM\n",
    "    return ssim(img1, img2, data_range=img1.max() - img1.min(), win_size=3)\n",
    "\n",
    "# Calculate SSIM\n",
    "ssim_value = calculate_ssim(generated_image, ground_truth_image)\n",
    "print(f\"SSIM between generated image and ground truth: {ssim_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b393147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR between generated image and ground truth: 6.6368\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate PSNR\n",
    "psnr_value = psnr(generated_image, ground_truth_image)\n",
    "print(f\"PSNR between generated image and ground truth: {psnr_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82606a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE between generated image and ground truth: 107.2621\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate MSE\n",
    "mse_value = mse(generated_image.flatten(), ground_truth_image.flatten())\n",
    "print(f\"MSE between generated image and ground truth: {mse_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab0797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
